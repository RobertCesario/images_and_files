{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87067bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import random\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models, regularizers\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceba801",
   "metadata": {},
   "source": [
    "### Reads the Excel sheets and loads the indices as DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cbc167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "desert_indices = pd.read_excel(\"Biome_tags_Spreadsheet.xlsx\", \"Robert\", header=None, usecols=[0])\n",
    "aquatic_indices = pd.read_excel(\"Biome_tags_Spreadsheet.xlsx\", \"Nathan\", header=None ,usecols=[0])\n",
    "tundra_indices = pd.read_excel(\"Biome_tags_Spreadsheet.xlsx\", \"Leo\", header=None, usecols=[0])\n",
    "forest_indices = pd.read_excel(\"Biome_tags_Spreadsheet.xlsx\", \"Jacob\", header=None, usecols=[0])\n",
    "grassland_indices = pd.read_excel(\"Biome_tags_Spreadsheet.xlsx\", \"Arkesh\", header=None, usecols=[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd304f7e",
   "metadata": {},
   "source": [
    "### Making lists of indices and tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "328e969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "desert_indices = desert_indices.to_numpy().astype(int).T.tolist()[0]\n",
    "aquatic_indices = aquatic_indices.to_numpy().astype(int).T.tolist()[0]\n",
    "tundra_indices = tundra_indices.to_numpy().astype(int).T.tolist()[0]\n",
    "forest_indices = forest_indices.to_numpy().astype(int).T.tolist()[0]\n",
    "grassland_indices = grassland_indices.to_numpy().astype(int).T.tolist()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a453d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "desert_type = (0*np.ones(len(desert_indices))).tolist()\n",
    "aquatic_type = (1*np.ones(len(aquatic_indices))).tolist()\n",
    "tundra_type = (2*np.ones(len(tundra_indices))).tolist()\n",
    "forest_type = (3*np.ones(len(forest_indices))).tolist()\n",
    "grassland_type = (4*np.ones(len(grassland_indices))).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a55447b",
   "metadata": {},
   "source": [
    "### Adds the indices and types into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "269c6be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = desert_indices + aquatic_indices + tundra_indices + forest_indices + grassland_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "563d5842",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1089"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indices) ## should match len(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ea585eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "types = desert_type + aquatic_type + tundra_type + forest_type + grassland_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0713ee2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1089"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(types) ## should match len(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5b6d6c",
   "metadata": {},
   "source": [
    "### Function for Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11afae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_images(indices, types, image_loc):\n",
    "    images = {}\n",
    "    names = []\n",
    "    \n",
    "    for i in range(len(indices)):\n",
    "        image_index = \"{:07d}\".format(indices[i])\n",
    "        image_name = \"img_\" + image_index\n",
    "        names.append(image_name)\n",
    "        image = np.array(Image.open(image_loc + \"\\\\\" + image_index + \".png\")) / 255\n",
    "        images[image_name] = [image, types[i]]\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c037c32",
   "metadata": {},
   "source": [
    "### Splitting into testing and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88bdf255",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list(zip(desert_indices, desert_type))\n",
    "des_train, des_test = train_test_split(pairs, test_size=0.2)\n",
    "des_train_ind, des_train_typ = zip(*des_train)\n",
    "des_test_ind, des_test_typ = zip(*des_test) \n",
    "\n",
    "pairs = list(zip(aquatic_indices, aquatic_type))\n",
    "aqu_train, aqu_test = train_test_split(pairs, test_size=0.2)\n",
    "aqu_train_ind, aqu_train_typ = zip(*aqu_train)\n",
    "aqu_test_ind, aqu_test_typ = zip(*aqu_test)\n",
    "\n",
    "pairs = list(zip(tundra_indices, tundra_type))\n",
    "tun_train, tun_test = train_test_split(pairs, test_size=0.2)\n",
    "tun_train_ind, tun_train_typ = zip(*tun_train)\n",
    "tun_test_ind, tun_test_typ = zip(*tun_test)\n",
    "\n",
    "pairs = list(zip(forest_indices, forest_type))\n",
    "for_train, for_test = train_test_split(pairs, test_size=0.2)\n",
    "for_train_ind, for_train_typ = zip(*for_train)\n",
    "for_test_ind, for_test_typ = zip(*for_test) \n",
    "\n",
    "pairs = list(zip(grassland_indices, grassland_type))\n",
    "gra_train, gra_test = train_test_split(pairs, test_size=0.2)\n",
    "gra_train_ind, gra_train_typ = zip(*gra_train)\n",
    "gra_test_ind, gra_test_typ = zip(*gra_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d418c74f",
   "metadata": {},
   "source": [
    "### Loading Images and preparing train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c63dc60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_loc = \"D:\\Dataset Images\\lhq_256\"\n",
    "training_loop = [[des_train_ind,des_train_typ],[aqu_train_ind, aqu_train_typ],[tun_train_ind, tun_train_typ],[for_train_ind, for_train_typ],[gra_train_ind, gra_train_typ]]\n",
    "testing_loop = [[des_test_ind, des_test_typ],[aqu_test_ind, aqu_test_typ],[tun_test_ind, tun_test_typ],[for_test_ind, for_test_typ],[gra_test_ind, gra_test_typ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "addfd896",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {}\n",
    "test_dict = {}\n",
    "for i in range(len(training_loop)):\n",
    "    train_dict.update(load_label_images(training_loop[i][0], training_loop[i][1], image_loc))\n",
    "\n",
    "for i in range(len(testing_loop)):\n",
    "    test_dict.update(load_label_images(testing_loop[i][0], testing_loop[i][1], image_loc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6039c152",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_dict).T\n",
    "train_df.columns = ['image', 'label']\n",
    "\n",
    "test_df = pd.DataFrame(test_dict).T\n",
    "test_df.columns = ['image', 'label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c93f6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = np.array(train_df['image'].tolist())\n",
    "train_lab = np.array(train_df['label'].tolist())\n",
    "test_img = np.array(test_df['image'].tolist())\n",
    "test_lab = np.array(test_df['label'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad80d24",
   "metadata": {},
   "source": [
    "### Tensorboard for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "80bc2fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "log_dir = 'D:/logs/hyperparams'\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "hparams_dir = os.path.join(log_dir, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7315747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_L2 = hp.HParam('regularizer', hp.RealInterval(0.005, 0.5))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.1, 0.5))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer('D:/logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "    hparams=[HP_L2, HP_DROPOUT, HP_OPTIMIZER],\n",
    "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Validation Accuracy')],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0957072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(hparams[HP_L2])),\n",
    "        layers.Dropout(hparams[HP_DROPOUT]),\n",
    "        layers.Dense(5, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(optimizer=hparams[HP_OPTIMIZER],\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[METRIC_ACCURACY])\n",
    "    model.fit(train_img, train_lab, epochs=1, validation_data=(test_img, test_lab))\n",
    "    _, accuracy = model.evaluate(test_img, test_lab)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a58a9c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        accuracy = train_test_model(hparams)\n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dc44c8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'regularizer': 0.005, 'dropout': 0.1, 'optimizer': 'adam'}\n",
      "28/28 [==============================] - 179s 6s/step - loss: 2.5331 - accuracy: 0.3048 - val_loss: 1.9060 - val_accuracy: 0.4028\n",
      "7/7 [==============================] - 13s 2s/step - loss: 1.9060 - accuracy: 0.4028\n",
      "--- Starting trial: run-1\n",
      "{'regularizer': 0.005, 'dropout': 0.1, 'optimizer': 'sgd'}\n",
      "28/28 [==============================] - 163s 6s/step - loss: 2.2643 - accuracy: 0.2298 - val_loss: 2.2644 - val_accuracy: 0.1991\n",
      "7/7 [==============================] - 8s 1s/step - loss: 2.2644 - accuracy: 0.1991\n",
      "--- Starting trial: run-2\n",
      "{'regularizer': 0.005, 'dropout': 0.30000000000000004, 'optimizer': 'adam'}\n",
      "28/28 [==============================] - 174s 6s/step - loss: 2.5274 - accuracy: 0.2852 - val_loss: 1.7829 - val_accuracy: 0.4259\n",
      "7/7 [==============================] - 10s 1s/step - loss: 1.7829 - accuracy: 0.4259\n",
      "--- Starting trial: run-3\n",
      "{'regularizer': 0.005, 'dropout': 0.30000000000000004, 'optimizer': 'sgd'}\n",
      "24/28 [========================>.....] - ETA: 7:58 - loss: 2.2288 - accuracy: 0.2630 "
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "{{function_node __wrapped__FlushSummaryWriter_device_/job:localhost/replica:0/task:0/device:CPU:0}} FlushFileBuffers failed for: \\\\?\\D:\\logs\\run-3\\events.out.tfevents.1712246591.BOB_INSPIRON.7352.28.v2 : The volume for a file has been externally altered so that the opened file is no longer valid.\r\n; Unknown error\n\tFailed to flush 1 events to D:/logs/run-3/events.out.tfevents.1712246591.BOB_INSPIRON.7352.28.v2\n\tCould not flush events file. [Op:FlushSummaryWriter] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--- Starting trial: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m run_name)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m({h\u001b[38;5;241m.\u001b[39mname: hparams[h] \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hparams})\n\u001b[1;32m---> 14\u001b[0m run(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/logs/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m run_name, hparams)\n\u001b[0;32m     15\u001b[0m session_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[80], line 2\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(run_dir, hparams)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(run_dir, hparams):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39msummary\u001b[38;5;241m.\u001b[39mcreate_file_writer(run_dir)\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[0;32m      3\u001b[0m         hp\u001b[38;5;241m.\u001b[39mhparams(hparams)  \u001b[38;5;66;03m# record the values used in this trial\u001b[39;00m\n\u001b[0;32m      4\u001b[0m         accuracy \u001b[38;5;241m=\u001b[39m train_test_model(hparams)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:95\u001b[0m, in \u001b[0;36m_SummaryContextManager.__exit__\u001b[1;34m(self, *exc)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mexc):\n\u001b[0;32m     93\u001b[0m   \u001b[38;5;66;03m# Flushes the summary writer in eager mode or in graph functions, but\u001b[39;00m\n\u001b[0;32m     94\u001b[0m   \u001b[38;5;66;03m# not in legacy graph mode (you're on your own there).\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m   _summary_state\u001b[38;5;241m.\u001b[39mwriter\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m     96\u001b[0m   _summary_state\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_old_writer\n\u001b[0;32m     97\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:359\u001b[0m, in \u001b[0;36m_ResourceSummaryWriter.flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    357\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu:0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 359\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m gen_summary_ops\u001b[38;5;241m.\u001b[39mflush_summary_writer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resource)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_summary_ops.py:199\u001b[0m, in \u001b[0;36mflush_summary_writer\u001b[1;34m(writer, name)\u001b[0m\n\u001b[0;32m    197\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 199\u001b[0m   _ops\u001b[38;5;241m.\u001b[39mraise_from_not_ok_status(e, name)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[0;32m    201\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mUnknownError\u001b[0m: {{function_node __wrapped__FlushSummaryWriter_device_/job:localhost/replica:0/task:0/device:CPU:0}} FlushFileBuffers failed for: \\\\?\\D:\\logs\\run-3\\events.out.tfevents.1712246591.BOB_INSPIRON.7352.28.v2 : The volume for a file has been externally altered so that the opened file is no longer valid.\r\n; Unknown error\n\tFailed to flush 1 events to D:/logs/run-3/events.out.tfevents.1712246591.BOB_INSPIRON.7352.28.v2\n\tCould not flush events file. [Op:FlushSummaryWriter] name: "
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "\n",
    "for l2 in np.linspace(0.005, 0.5, 3):\n",
    "    for dropout_rate in np.linspace(0.1, 0.5, 3):\n",
    "        for optimizer in HP_OPTIMIZER.domain.values:\n",
    "            hparams = {\n",
    "                  HP_L2: l2,\n",
    "                  HP_DROPOUT: dropout_rate,\n",
    "                  HP_OPTIMIZER: optimizer,\n",
    "            }\n",
    "            run_name = \"run-%d\" % session_num\n",
    "            print('--- Starting trial: %s' % run_name)\n",
    "            print({h.name: hparams[h] for h in hparams})\n",
    "            run('D:/logs/' + run_name, hparams)\n",
    "            session_num += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
